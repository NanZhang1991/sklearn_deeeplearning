{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章中我们会讨论一下特别著名的集成方法，包括 **bagging, boosting, stacking**，和其他一些算法。我们也会讨论随机森林。\n",
    "\n",
    "## 投票分类\n",
    "假设你已经训练了一些分类器，每一个都有 80% 的准确率。你可能有了一个逻辑回归、或一个 SVM、或一个随机森林，或者一个 KNN分类器，一个非常简单的创建一个更好的分类器的方法就是去整合每一个分类器的预测然后经过投票预测分类。这种分类器叫做硬投票分类器。\n",
    "\n",
    "然而每一个分类器都在同一个数据集上训练，导致其很可能会发生这样的错误。他们可能会犯同一种错误，所以也会有很多票投给了错误类别导致集成的准确率下降。\n",
    "\n",
    "如果使每一个分类器都独立自主的分类，那么集成模型会工作的很好。得到多样的分类器的方法之一就是用完全不同的算法，这会使它们会做出不同种类的错误，但会提高集成的正确率。\n",
    "接下来的代码创建和训练了在 sklearn 中的投票分类器。这个分类器由三个不同的分类器组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "X,y = iris_data.data, iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial')),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC()),\n",
       "                             ('pc',\n",
       "                              Pipeline(steps=(('poly_features',\n",
       "                                               PolynomialFeatures(degree=3)),\n",
       "                                              ('scaler', StandardScaler()),\n",
       "                                              ('svm_clf',\n",
       "                                               LinearSVC(C=10,\n",
       "                                                         loss='hinge')))))])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=42)\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\") \n",
    "rnd_clf = RandomForestClassifier() \n",
    "svm_clf = SVC()\n",
    "polynomial_svm_clf = Pipeline((\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "    ))\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf),\n",
    "                                         ('pc',polynomial_svm_clf,)],voting='hard') \n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 1.0\n",
      "RandomForestClassifier 0.98\n",
      "Pipeline 0.98\n",
      "SVC 1.0\n",
      "VotingClassifier 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "for clf in (log_clf, rnd_clf, polynomial_svm_clf, svm_clf, voting_clf): \n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test) \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果所有的分类器都能够预测类别的概率（例如他们有一个predict_proba()方法），那么你就可以让 sklearn 以最高的类概率来预测这个类，平均在所有的分类器上。这种方式叫做软投票。他经常比硬投票表现的更好，因为它给予高自信的投票更大的权重。你可以通过把voting=\"hard\"设置为voting=\"soft\"来保证分类器可以预测类别概率。然而这不是 SVC 类的分类器默认的选项，所以你需要把它的probability hyperparameter设置为True（这会使 SVC 使用交叉验证去预测类别概率，其降低了训练速度，但会添加predict_proba()方法）。如果你修改了之前的代码去使用软投票，你会发现投票分类器正确率高达 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 和 Pasting\n",
    "就像之前讲到的，可以通过使用不同的训练算法去得到一些不同的分类器。另一种方法就是对每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练它们。有放回采样被称为装袋（Bagging，是 bootstrap aggregating 的缩写）。无放回采样称为粘贴（pasting）。\n",
    "\n",
    "换句话说，Bagging 和 Pasting 都允许在多个分类器上对训练集进行多次采样，但只有 Bagging 允许对同一种分类器上对训练集进行进行多次采样。采样和训练过程如图7-4所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当所有的分类器被训练后，集成可以通过对所有分类器结果的简单聚合来对新的实例进行预测。聚合函数通常对分类是统计模式（例如硬投票分类器）或者对回归是平均。每一个单独的分类器在如果在原始训练集上都是高偏差，但是聚合降低了偏差和方差。通常情况下，集成的结果是有一个相似的偏差，但是对比与在原始训练集上的单一分类器来讲有更小的方差。\n",
    "\n",
    "**在 sklearn 中的 Bagging 和 Pasting**<br>\n",
    "sklearn 为 Bagging 和 Pasting 提供了一个简单的API：BaggingClassifier类（或者对于回归可以是BaggingRegressor。接下来的代码训练了一个 500 个决策树分类器的集成，每一个都是在数据集上有放回采样 100 个训练实例下进行训练（这是 Bagging 的例子，如果你想尝试 Pasting，就设置bootstrap=False）。n_jobs参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量。（-1 代表着 sklearn 会使用所有空闲核）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1) \n",
    "bag_clf.fit(X_train, y_train)              \n",
    "y_pred = bag_clf.predict(X_test)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笔记<br>\n",
    "如果基分类器可以预测类别概率（例如它拥有predict_proba()方法），那么BaggingClassifier会自动的运行软投票，这是决策树分类器的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out-of-Bag 评价**<br>\n",
    "\n",
    "对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。BaggingClassifier默认采样。BaggingClassifier默认是有放回的采样m个实例 （bootstrap=True），其中m是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 Out-of-Bag 实例。注意对于每一个的分类器它们的 37% 不是相同的。\n",
    "\n",
    "因为在训练中分类器从来没有看到过 oob 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。\n",
    "\n",
    "在 sklearn 中，你可以在训练后需要创建一个BaggingClassifier时设置oob_score=True来自动评估。接下来的代码展示了这个操作。评估结果通过变量oob_score_来显示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train) \n",
    "bag_clf.oob_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = bag_clf.predict(X_test) \n",
    "accuracy_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个训练实例 oob 决策函数也可通过oob_decision_function_变量来展示。在这种情况下（当基决策器有predict_proba()时）决策函数会对每个训练实例返回类别概率。例如，oob 评估预测第二个训练实例有 60.6% 的概率属于正类（39.4% 属于负类):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机贴片与随机子空间\n",
    "BaggingClassifier也支持采样特征。它被两个超参数max_features和bootstrap_features控制。他们的工作方式和max_samples和bootstrap一样，但这是对于特征采样而不是实例采样。因此，每一个分类器都会被在随机的输入特征内进行训练。\n",
    "\n",
    "当你在处理高维度输入下（例如图片）此方法尤其有效。对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如bootstrap=False和max_samples=1.0），但是对特征采样（bootstrap_features=True并且/或者max_features小于 1.0）叫做随机子空间。\n",
    "\n",
    "采样特征导致更多的预测多样性，用高偏差换低方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林\n",
    "正如我们所讨论的，随机森林是决策树的一种集成，通常是通过 bagging 方法（有时是 pasting 方法）进行训练，通常用max_samples设置为训练集的大小。建立一个BaggingClassifier，然后把它放入 DecisionTreeClassifier 。或者，你可以使用更方便的也是对决策树优化过的RandomForestClassifier（对于回归是RandomForestRegressor）。接下来的代码训练了带有 500 个树（每个被限制为 16 叶子结点）的决策森林，使用所有空闲的 CPU 核：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) \n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "y_pred_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了一些例外，RandomForestClassifier使用DecisionTreeClassifier的所有超参数（决定树怎么生长），把BaggingClassifier的超参数加起来来控制集成本身。\n",
    "\n",
    "随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反（详见第六章），它在一个随机的特征集中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。以下是BaggingClassifier，大致相当于之前的randomforestclassifier："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_leaf_nodes=16,\n",
       "                                                        splitter='random'),\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极端随机树\n",
    "当你在随机森林上生长树时，在每个结点分裂时只考虑随机特征集上的特征（正如之前讨论过的一样）。相比于找到更好的特征我们可以通过使用对特征使用随机阈值使树更加随机（像规则决策树一样）。\n",
    "\n",
    "这种极端随机的树被简称为 Extremely Randomized Trees（极端随机树），或者更简单的称为 Extra-Tree。再一次用高偏差换低方差。它还使得 Extra-Tree 比规则的随机森林更快地训练，因为在每个节点上找到每个特征的最佳阈值是生长树最耗时的任务之一。\n",
    "\n",
    "你可以使用 sklearn 的ExtraTreesClassifier来创建一个 Extra-Tree 分类器。他的 API 跟RandomForestClassifier是相同的，相似的， ExtraTreesRegressor 跟RandomForestRegressor也是相同的 API。\n",
    "\n",
    "我们很难去分辨ExtraTreesClassifier和RandomForestClassifier到底哪个更好。通常情况下是通过交叉验证来比较它们（使用网格搜索调整超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "X,y = iris_data.data, iris_data.target\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)\n",
    "ExtraTreesClassifier(random_state=0)\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 孤立森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# 生成训练数据\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 1, X - 3, X - 5, X + 6]\n",
    "\n",
    "# 生成正常数据\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 1, X - 3, X - 5, X + 6]\n",
    "\n",
    "# 生成异常数据\n",
    "X_outliers = rng.uniform(low=-8, high=8, size=(20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集:[-1  1]\n",
      "测试集: [ 1 -1]\n",
      "异常:[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用模型\n",
    "clf = IsolationForest(max_samples=100*2, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print('训练集:{}'.format(y_pred_train[3:5]))\n",
    "y_pred_test = clf.predict(X_test)\n",
    "print('测试集: {}'.format(y_pred_test[3:5]))\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "print('异常:{}'.format(y_pred_outliers))\n",
    " \n",
    "# 作图\n",
    "xx, yy = np.meshgrid(np.linspace(-8, 8, 50), np.linspace(-8, 8, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-8, 8))\n",
    "plt.ylim((-8, 8))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要度\n",
    "最后，如果你观察一个单一决策树，重要的特征会出现在更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。sklearn 在训练后会自动计算每个特征的重要度。你可以通过feature_importances_变量来查看结果。例如如下代码在 iris 数据集（第四章介绍）上训练了一个RandomForestClassifier模型，然后输出了每个特征的重要性。看来，最重要的特征是花瓣长度（44%）和宽度（42%），而萼片长度和宽度相对比较是不重要的（分别为 11% 和 2%）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09444136588196106\n",
      "sepal width (cm) 0.024272040345556155\n",
      "petal length (cm) 0.45002399012837063\n",
      "petal width (cm) 0.4312626036441123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1) \n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"]) \n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_): \n",
    "    print(name, score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提升\n",
    "提升（Boosting，最初称为假设增强）指的是可以将几个弱学习者组合成强学习者的集成方法。对于大多数的提升方法的思想就是按顺序去训练分类器，每一个都要尝试修正前面的分类。现如今已经有很多的提升方法了，但最著名的就是 Adaboost（适应性提升，是 Adaptive Boosting 的简称） 和 Gradient Boosting（梯度提升）。让我们先从 Adaboost 说起。\n",
    "\n",
    "## Adaboost\n",
    "使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练实例多加关注。这导致新的预测因子越来越多地聚焦于这种情况。这是 Adaboost 使用的技术。\n",
    "\n",
    "举个例子，去构建一个 Adaboost 分类器，第一个基分类器（例如一个决策树）被训练然后在训练集上做预测，在误分类训练实例上的权重就增加了。第二个分类机使用更新过的权重然后再一次训练，权重更新，以此类推<br>\n",
    "警告<br>\n",
    "序列学习技术的一个重要的缺点就是：它不能被并行化（只能按步骤），因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练。因此，它不像Bagging和Pasting一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn 通常使用 Adaboost 的多分类版本 SAMME（这就代表了 分段加建模使用多类指数损失函数）。如果只有两类别，那么 SAMME 是与 Adaboost 相同的。如果分类器可以预测类别概率（例如如果它们有predict_proba()），如果 sklearn 可以使用 SAMME 叫做SAMME.R的变量（R 代表“REAL”），这种依赖于类别概率的通常比依赖于分类器的更好。\n",
    "\n",
    "接下来的代码训练了使用 sklearn 的AdaBoostClassifier基于 200 个决策树桩的Adaboost 分类器（正如你说期待的，对于回归也有AdaBoostRegressor）。一个决策树桩是max_depth=1的决策树-换句话说，是一个单一的决策节点加上两个叶子结点。这就是AdaBoostClassifier的默认基分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm=\"SAMME.R\", learning_rate=0.5) \n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你的 Adaboost 集成过拟合了训练集，你可以尝试减少基分类器的数量或者对基分类器使用更强的正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度提升\n",
    "另一个非常著名的提升算法是梯度提升。与 Adaboost 一样，梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。然而，它并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法是去使用新的分类器去拟合前面分类器预测的残差 。\n",
    "\n",
    "让我们通过一个使用决策树当做基分类器的简单的回归例子（回归当然也可以使用梯度提升）学习。这叫做梯度提升回归树（GBRT，Gradient Tree Boosting 或者 Gradient Boosted Regression Trees）。首先我们用DecisionTreeRegressor去拟合训练集（例如一个有噪二次训练集）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg1.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在第一个分类器的残差上训练第二个分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X) \n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后在第二个分类器的残差上训练第三个分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg1.predict(X) \n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了一个包含三个回归器的集成。它可以通过集成所有树的预测来在一个新的实例上进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08972811, -0.08972811,  0.03701831, -0.08972811, -0.08972811,\n",
       "        0.00676638, -0.08972811,  0.03701831, -0.08972811, -0.08972811,\n",
       "        0.03701831,  0.00676638, -0.08972811,  0.00676638,  0.00676638,\n",
       "       -0.08972811,  0.03701831, -0.08972811, -0.08972811,  0.03701831,\n",
       "        0.00676638, -0.08972811,  0.00676638,  0.03701831,  0.03701831,\n",
       "        0.03701831,  0.03701831,  0.03701831,  0.00676638,  0.00676638,\n",
       "        0.00676638, -0.08972811, -0.08972811,  0.00676638,  0.00676638,\n",
       "        0.03701831, -0.08972811,  0.00676638,  0.00676638,  0.00676638,\n",
       "        0.03701831, -0.08972811, -0.08972811,  0.00676638,  0.00676638,\n",
       "       -0.08972811,  0.90740741,  0.03701831, -0.08972811,  0.03701831])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3)) \n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以它与第一个树的预测相似。在第二行，一个新的树在第一个树的残差上进行训练。在右边栏可以看出集成的预测等于前两个树预测的和。相同的，在第三行另一个树在第二个数的残差上训练。你可以看到集成的预测会变的更好。\n",
    "\n",
    "我们可以使用 sklean 中的GradientBoostingRegressor来训练 GBRT 集成。与RandomForestClassifier相似，它也有超参数去控制决策树的生长（例如max_depth，min_samples_leaf等等），也有超参数去控制集成训练，例如基分类器的数量（n_estimators）。接下来的代码创建了与之前相同的集成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99103926, -0.10155333,  2.        ,  0.99103926,  0.99103926,\n",
       "       -0.00505885,  0.99103926,  2.00345395,  0.99103926,  0.99103926,\n",
       "        2.00345395,  0.03703704, -0.10155333,  0.03703704, -0.00505885,\n",
       "        0.99103926,  2.00345395,  0.99103926,  0.99103926,  2.00345395,\n",
       "        0.03703704,  1.87670754, -0.00505885,  2.00345395,  2.        ,\n",
       "        2.00345395,  2.00345395,  2.00345395,  0.03703704,  0.03703704,\n",
       "        0.03703704, -0.10155333,  0.99103926,  0.03703704,  0.03703704,\n",
       "        2.00345395,  0.99103926, -0.00505885, -0.00505885, -0.00505885,\n",
       "        2.00345395,  0.99103926,  0.99103926, -0.00505885, -0.00505885,\n",
       "        0.99103926,  1.98817477,  2.00345395,  0.99103926,  2.        ])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) \n",
    "gbrt.fit(X, y)\n",
    "gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了找到树的最优数量，你可以使用早停技术（第四章讨论过）。最简单使用这个技术的方法就是使用staged_predict()：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数learning_rate 确立了每个树的贡献。如果你把它设置为一个很小的树，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 shrinkage。图 7-10 展示了两个在低学习率上训练的 GBRT 集成：其中左侧是一个没有足够树去拟合训练集的树，右侧是有过多的树过拟合训练集的树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了找到树的最优数量，你可以使用早停技术（第四章讨论过）。最简单使用这个技术的方法就是使用staged_predict()：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=60)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) \n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)         \n",
    "     for y_pred in gbrt.staged_predict(X_val)] \n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) \n",
    "gbrt_best.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以早早的停止训练来实现早停（而不是先在一大堆树中训练，然后再回头去找最佳数量）。你可以通过设置warm_start=True来实现 ，这使得当fit()方法被调用时 sklearn 保留现有树，并允许增量训练。接下来的代码在当一行中的五次迭代验证错误没有改善时会停止训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\") \n",
    "error_going_up = 0 \n",
    "for n_estimators in range(1, 120):    \n",
    "    gbrt.n_estimators = n_estimators    \n",
    "    gbrt.fit(X_train, y_train)    \n",
    "    y_pred = gbrt.predict(X_val)    \n",
    "    val_error = mean_squared_error(y_val, y_pred)    \n",
    "    if val_error < min_val_error:        \n",
    "        min_val_error = val_error        \n",
    "        error_going_up = 0    \n",
    "    else:        \n",
    "        error_going_up += 1        \n",
    "        if error_going_up == 5:            \n",
    "            break  # early stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost和GBDT的区别\n",
    "将树模型的复杂度加入到正则项中，来避免过拟合，因此泛化性能会由于GBDT。\n",
    "\n",
    "　  2）损失函数是用泰勒展开式展开的，同时用到了一阶导和二阶导，可以加快优化速度。\n",
    "\n",
    "　　3）和GBDT只支持CART作为基分类器之外，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化。\n",
    "\n",
    "　　4）引进了特征子采样，像RandomForest那样，这种方法既能降低过拟合，还能减少计算。\n",
    "\n",
    "　　5）在寻找最佳分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减小内存消耗，除此之外还考虑了稀疏数据集和缺失值的处理，对于特征的值有缺失的样本，XGBoost依然能自动找到其要分裂的方向。\n",
    "\n",
    "　　6）XGBoost支持并行处理，XGBoost的并行不是在模型上的并行，而是在特征上的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。这个block也使得并行化成为了可能，其次在进行节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。\n",
    "\n",
    "分类: 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_errorGradientBoostingRegressor也支持指定用于训练每棵树的训练实例比例的超参数subsample。例如如果subsample=0.25，那么每个树都会在 25% 随机选择的训练实例上训练。你现在也能猜出来，这也是个高偏差换低方差的作用。它同样也加速了训练。这个技术叫做随机梯度提升。\n",
    "\n",
    "也可能对其他损失函数使用梯度提升。这是由损失超参数控制（见 sklearn 文档）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "本章讨论的最后一个集成方法叫做 Stacking（stacked generalization 的缩写）。这个算法基于一个简单的想法：不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我们为什么不训练一个模型来执行这个聚合？图 7-12 展示了这样一个在新的回归实例上预测的集成。底部三个分类器每一个都有不同的值（3.1，2.7 和 2.9），然后最后一个分类器（叫做 blender 或者 meta learner ）把这三个分类器的结果当做输入然后做出最终决策（3.0）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
